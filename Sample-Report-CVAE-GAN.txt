

üìÑ Final Project Report (Detailed Version)

Title: Conditional VAE‚ÄìGAN for Class-Conditional Image Generation on CIFAR-100

‚∏ª

1. Introduction

Generative modeling aims to learn the underlying structure of a dataset in a way that enables the synthesis of new samples that look similar to those in the dataset. Classical generative models such as Gaussian mixtures or PCA-based approaches have limited capacity to model complex high-dimensional data such as natural images.

Modern deep generative models ‚Äî including Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) ‚Äî significantly extend these capabilities. However, each approach has complementary strengths and weaknesses:
	‚Ä¢	VAEs learn smooth latent spaces and produce stable reconstructions, but often yield blurry images.
	‚Ä¢	GANs produce sharp and realistic images, but lack an encoder and often suffer from training instability.

In this project, we combine both approaches into a Conditional VAE‚ÄìGAN (CVAE-GAN) that:
	1.	Learns a latent representation (pattern recognition)
	2.	Reconstructs input images (autoencoding)
	3.	Generates realistic samples conditioned on class labels (generative modeling)

We use the CIFAR-100 dataset, which is challenging due to its small resolution (32√ó32) and 100 diverse classes.
Our system successfully trains a stable hybrid model capable of:
	‚Ä¢	Class-conditional generation
	‚Ä¢	Reconstruction of real images
	‚Ä¢	Smooth latent interpolations
	‚Ä¢	Organized latent representations

This demonstrates the core principles of representation learning and generative modeling discussed in class.

‚∏ª

2. Dataset

We use the CIFAR-100 dataset:
	‚Ä¢	100 classes
	‚Ä¢	50,000 training images, 10,000 test images
	‚Ä¢	Resolution: 32√ó32 pixels, RGB
	‚Ä¢	High intra-class variation and low resolution make it a difficult dataset for generative models.

All images are:
	‚Ä¢	scaled to [0, 1]
	‚Ä¢	normalized to [-1, 1]
	‚Ä¢	labels encoded as integers and embedded for conditional modeling

Normalization is essential because both VAEs and GANs assume the input distribution is approximately bounded and symmetric.

‚∏ª

3. Model Architecture

Our hybrid model consists of four main components. This section explains each one in plain yet technically precise terms.

‚∏ª

3.1 Encoder (VAE component)

The encoder‚Äôs job is to take an image x and its class label y, and compress them into a latent distribution instead of a single vector. Rather than outputting just one latent vector, it outputs:
	‚Ä¢	\mu(x, y): the mean of the latent Gaussian
	‚Ä¢	\log \sigma^2(x, y): the log-variance

This lets the model learn uncertainty about how to encode images.

Why a distribution?
Because VAEs encourage the latent space to be smooth and continuous ‚Äî small changes in latent space should produce small changes in the generated image.

‚∏ª

3.2 Reparameterization Trick

Instead of sampling directly from the Gaussian (which would break backpropagation), we use:

z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)

This keeps sampling differentiable.
It also means that:
	‚Ä¢	the encoder learns which features matter
	‚Ä¢	random noise injects generative diversity

‚∏ª

3.3 Generator (Decoder)

The generator takes:
	‚Ä¢	latent vector z
	‚Ä¢	class label y

and synthesizes a 32√ó32 RGB image.

The class label is fed into the generator using an embedding, so the model can gradually learn ‚Äúclass style.‚Äù For example:
	‚Ä¢	apples ‚Üí red round shapes
	‚Ä¢	birds ‚Üí feather-like textures
	‚Ä¢	landscapes ‚Üí blue/green patterns

The generator is trained with:
	‚Ä¢	reconstruction loss
	‚Ä¢	adversarial loss

so it learns both accuracy and realism.

‚∏ª

3.4 Discriminator (GAN component)

The discriminator receives:
	‚Ä¢	an image x (real or generated)
	‚Ä¢	a class label y

and must output:
	‚Ä¢	1 if the image is real
	‚Ä¢	0 if it is generated

This forces the generator to produce more realistic images.

We use a projection-based discriminator, which is known to stabilize conditional GANs.

‚∏ª

4. Loss Functions

Our total loss combines VAE and GAN components.

\mathcal{L} = \mathcal{L}_{\text{recon}} +
\beta \cdot \mathcal{L}_{\text{KL}} +
\lambda \cdot \mathcal{L}_{\text{GAN}}

Where:

‚∏ª

4.1 Reconstruction Loss

We use pixel-wise MSE:

\mathcal{L}_{\text{recon}} = ||x - \hat{x}||^2

This encourages the generator to rebuild the image faithfully.

‚∏ª

4.2 KL Divergence

Regularizes the encoder‚Äôs latent distribution to stay close to a unit Gaussian:

\mathcal{L}_{\text{KL}} =
\frac{1}{2}\sum(\mu^2 + \sigma^2 - \log\sigma^2 - 1)

If the KL term collapses to 0, the encoder stops using latent space.
If KL grows too large, the latent space becomes unstable.

We use Œ≤ = 0.2, which stabilizes training.

‚∏ª

4.3 Adversarial Loss

This comes from the GAN game:
	‚Ä¢	Generator tries to fool the discriminator
	‚Ä¢	Discriminator tries to classify correctly

We use non-saturating GAN loss, which is stable.

A small weight Œª = 0.1 ensures the GAN sharpens images without overpowering the VAE.

‚∏ª

5. Training Procedure
	‚Ä¢	Device: Apple MPS
	‚Ä¢	Epochs: 20
	‚Ä¢	Batch size: 128
	‚Ä¢	Optimizer: Adam
	‚Ä¢	Generator LR: 2e-4
	‚Ä¢	Discriminator LR: 1e-4
	‚Ä¢	Label embeddings for all conditional components
	‚Ä¢	Mixed VAE‚ÄìGAN loss schedule applied consistently

Training stabilizes after ~5 epochs and remains stable to epoch 20.

‚∏ª

6. Results

Here we describe and interpret each of the results the model produced.

‚∏ª

6.1 Loss Curves

Observation:
	‚Ä¢	KL loss slowly decreased ‚Üí encoder is learning a compact representation
	‚Ä¢	Reconstruction loss stable around 0.39‚Äì0.43 ‚Üí indicates consistency
	‚Ä¢	Discriminator loss decreased from ~2.6 ‚Üí ~1.1 ‚Üí healthy balance
	‚Ä¢	Generator loss oscillated between 5‚Äì8 ‚Üí standard GAN behavior

Interpretation:
	‚Ä¢	No mode collapse
	‚Ä¢	No divergence
	‚Ä¢	The adversarial and VAE components are well balanced

This confirms a successful training run.

‚∏ª

6.2 Class-Conditional Generation

We generated samples from classes 0‚Äì7.

Observation:
	‚Ä¢	Each row exhibits consistent texture/patterns
	‚Ä¢	Apple-like shapes appear in class 0
	‚Ä¢	Bird/animal/landscape-ish textures appear in other rows
	‚Ä¢	Within each row, samples vary but preserve class style

Interpretation:
This demonstrates that the generator is able to:
	1.	Use the class label
	2.	Produce diverse samples within a class
	3.	Learn coarse features even on low-resolution CIFAR-100

This is strong evidence of conditional generative capability.

‚∏ª

6.3 Reconstruction Results

We compare real validation images with reconstructions.

Observation:
	‚Ä¢	Color distributions match
	‚Ä¢	Object positions roughly align
	‚Ä¢	Images are blurry (expected from VAE-based models)

Interpretation:
The encoder is capturing:
	‚Ä¢	global structure
	‚Ä¢	dominant colors
	‚Ä¢	scene layout

The generator can reconstruct these latent features, proving meaningful pattern recognition in the latent space.

Reconstruction MSE on a batch: 0.4103

‚∏ª

6.4 Latent Interpolation

We encoded two validation images, then interpolated between their latent vectors.

Observation:
	‚Ä¢	Smooth transition from one image‚Äôs color/texture to the other
	‚Ä¢	No abrupt artifacts

Interpretation:
	‚Ä¢	Latent space is continuous
	‚Ä¢	Encoder has organized images into a manifold
	‚Ä¢	Small latent changes ‚Üí small visual changes

This is evidence of structured representation learning.

‚∏ª

7. Discussion

Our model successfully demonstrates:

VAE Strengths
	‚Ä¢	Organized latent space
	‚Ä¢	Smooth interpolations
	‚Ä¢	Meaningful encodings
	‚Ä¢	Stable reconstructions

GAN Strengths
	‚Ä¢	Class-conditional generation
	‚Ä¢	Sharper textures than pure VAEs
	‚Ä¢	Label control over output

Why CIFAR-100 is challenging
	‚Ä¢	32√ó32 resolution
	‚Ä¢	100 classes ‚Üí representation is very fragmented
	‚Ä¢	Complex textures compressed into tiny spatial dimensions

Given this difficulty, the results are entirely appropriate.

‚∏ª

8. Limitations & Future Work

Limitations
	‚Ä¢	Generated samples lack fine detail
	‚Ä¢	Reconstructions are blurry
	‚Ä¢	Training time long on MPS
	‚Ä¢	GAN loss still oscillates (normal but noisy)

Future Improvements
	‚Ä¢	Train for 40‚Äì60 epochs
	‚Ä¢	Pretrain VAE before enabling GAN
	‚Ä¢	Use perceptual loss (VGG-based)
	‚Ä¢	Use EMA (Exponential Moving Average) for generator
	‚Ä¢	Add VAE-only and GAN-only baselines for comparison
	‚Ä¢	Switch to larger architecture (ResNet-based)

‚∏ª

9. Conclusion

We implemented a full Conditional VAE‚ÄìGAN from scratch and demonstrated:
	‚Ä¢	Stable hybrid training
	‚Ä¢	Class-conditional sample generation
	‚Ä¢	Meaningful reconstructions
	‚Ä¢	Smooth latent space interpolations
	‚Ä¢	Use of conditional embeddings across encoder, generator, and discriminator

This project clearly illustrates core concepts in pattern recognition:
	1.	Latent feature learning (encoder)
	2.	Generative synthesis (decoder + GAN)
	3.	Discriminative modeling (discriminator)
	4.	Regularization and probabilistic representation (KL term)
	5.	Conditional modeling (labels)

The model achieves its goals and provides a complete demonstration of modern generative modeling applied to a multi-class dataset.

